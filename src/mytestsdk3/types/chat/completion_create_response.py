# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

import builtins
from typing import Dict, List, Union, Optional
from typing_extensions import Literal

from ..._models import BaseModel

__all__ = [
    "CompletionCreateResponse",
    "Choice",
    "ChoiceMessage",
    "ChoiceMessageFunctionCall",
    "ChoiceMessageToolCall",
    "ChoiceMessageToolCallFunction",
    "ChoiceLogprobs",
    "ChoiceLogprobsContent",
    "ChoiceLogprobsContentTopLogprob",
    "PromptLogprobPromptLogprobItem",
    "Usage",
    "UsagePromptTokensDetails",
]


class ChoiceMessageFunctionCall(BaseModel):
    arguments: str

    name: str


class ChoiceMessageToolCallFunction(BaseModel):
    arguments: str

    name: str


class ChoiceMessageToolCall(BaseModel):
    id: str
    """The ID of the tool call."""

    function: ChoiceMessageToolCallFunction

    type: Literal["function"]
    """The type of the tool. Currently, only function is supported."""


class ChoiceMessage(BaseModel):
    role: str
    """The role of the author of this message."""

    annotations: Optional[Dict[str, object]] = None
    """OpenAI annotation information."""

    audio: Optional[Dict[str, object]] = None
    """OpenAI chat completion audio information."""

    content: Optional[str] = None
    """The content of the message."""

    function_call: Optional[ChoiceMessageFunctionCall] = None
    """Deprecated function call information."""

    reasoning: Optional[str] = None
    """Reasoning content generated by the model."""

    reasoning_content: Optional[str] = None
    """Reasoning content generated by the model."""

    refusal: Optional[str] = None
    """Refusal message if the model refuses to respond."""

    tool_calls: Optional[List[ChoiceMessageToolCall]] = None
    """List of tool calls made by the model."""


class ChoiceLogprobsContentTopLogprob(BaseModel):
    token: str
    """The token."""

    logprob: float
    """The log probability of the token."""

    bytes: Optional[List[int]] = None
    """A list of integers representing the UTF-8 bytes representation of the token."""


class ChoiceLogprobsContent(BaseModel):
    token: str
    """The token."""

    logprob: float
    """The log probability of the token."""

    bytes: Optional[List[int]] = None
    """A list of integers representing the UTF-8 bytes representation of the token."""

    top_logprobs: Optional[List[ChoiceLogprobsContentTopLogprob]] = None
    """List of top log probabilities for alternative tokens."""


class ChoiceLogprobs(BaseModel):
    content: Optional[List[ChoiceLogprobsContent]] = None
    """A list of message content tokens with log probability information."""


class Choice(BaseModel):
    index: int
    """The index of the choice in the list of choices."""

    message: ChoiceMessage

    finish_reason: Optional[str] = None
    """The reason the model stopped generating tokens."""

    logprobs: Optional[ChoiceLogprobs] = None
    """Log probability information for the choice."""

    stop_reason: Union[int, str, None] = None
    """Not part of OpenAI spec but included for legacy reasons."""

    token_ids: Optional[List[int]] = None
    """Not part of OpenAI spec but useful for tracing tokens in agent scenarios."""


class PromptLogprobPromptLogprobItem(BaseModel):
    logprob: float
    """The logprob of chosen token."""

    decoded_token: Optional[str] = None
    """The decoded chosen token index"""

    rank: Optional[int] = None
    """The vocab rank of chosen token (>=1)."""


class UsagePromptTokensDetails(BaseModel):
    cached_tokens: Optional[int] = None
    """Number of tokens that were cached and reused."""


class Usage(BaseModel):
    prompt_tokens: int
    """Number of tokens in the prompt."""

    total_tokens: int
    """Total number of tokens used in the request (prompt + completion)."""

    completion_tokens: Optional[int] = None
    """Number of tokens in the generated completion."""

    prompt_tokens_details: Optional[UsagePromptTokensDetails] = None
    """Breakdown of tokens in the prompt."""


class CompletionCreateResponse(BaseModel):
    id: str
    """A unique identifier for the chat completion."""

    choices: List[Choice]
    """A list of chat completion choices.

    Can be more than one if `n` is greater than 1.
    """

    created: int
    """The Unix timestamp (in seconds) of when the chat completion was created."""

    model: str
    """The model used for the chat completion."""

    object: Literal["chat.completion"]
    """The object type, which is always `chat.completion`."""

    kv_transfer_params: Optional[Dict[str, builtins.object]] = None
    """KVTransfer parameters."""

    prompt_logprobs: Optional[List[Dict[str, PromptLogprobPromptLogprobItem]]] = None
    """
    Array of dictionaries or null values, where each dictionary maps token index (as
    string) to a Logprob object containing log probability, rank, and decoded token
    information. Each element can be a dictionary (dict[int, Logprob]) or null. The
    entire field can also be null. The length of this array should match the number
    of prompt tokens reported in the usage statistics.
    """

    prompt_token_ids: Optional[List[int]] = None
    """
    Array of token IDs representing how the prompt was tokenized by the model's
    tokenizer. Each integer corresponds to a token ID in the tokenizer's vocabulary.
    This field is useful for debugging tokenization, verifying token counts against
    `usage.prompt_tokens`, and performing token-level analysis. The length of this
    array should match the number of prompt tokens reported in the usage statistics.
    """

    service_tier: Optional[Literal["auto", "default", "flex", "scale", "priority"]] = None
    """The service tier used for the request."""

    system_fingerprint: Optional[str] = None
    """This fingerprint represents the backend configuration that the model runs with.

    Can be used in conjunction with the `seed` request parameter to understand when
    backend changes have been made that might impact determinism.
    """

    usage: Optional[Usage] = None
    """Usage statistics for the completion request."""
